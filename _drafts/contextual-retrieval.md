---
layout: post
title: "contextual retrieval 이 중요한 이유는 뭘까"
date: 2025-08-06 12:00:00 +0900
categories: [Work]
tags: [ai, context engineering, 개발, contextual retrieval]
---

context engineering 은 에이전틱 개발에서 중요한 부분 중에 하나다.
많은 양의 토큰을 사용하면 그만큼 맥락을 유지하면서 단계적으로 목표를 향해 결과를 찾아가야하기 때문에 적절한 컨텐스트를 어떻게 유지시키는가는 매우 중요한 문제다.

[Introducing Contextual Retrieval - ANTHROPIC](https://www.anthropic.com/engineering/contextual-retrieval){:target="_blank"}

이링크의 글을 상세히 분석한다.

Anthropic의 Contextual Retrieval 상세 분석
Anthropic이 발표한 이 기술 문서는 AI 시스템의 검색 증강 생성(RAG) 성능을 획기적으로 개선하는 "Contextual Retrieval" 방법론을 소개합니다. 주요 내용을 상세히 분석해드리겠습니다.
## 1. 핵심 문제와 해결책
문제점:

기존 RAG 시스템은 문서를 작은 청크로 나눌 때 중요한 맥락 정보를 잃어버림
예: "회사의 매출이 전 분기 대비 3% 성장했다"는 문장만으로는 어느 회사인지, 언제의 정보인지 알 수 없음

해결책: Contextual Retrieval

각 청크에 맥락 정보를 자동으로 추가하여 검색 정확도를 대폭 향상
검색 실패율을 49% 감소시키고, 리랭킹과 결합 시 67%까지 감소

## 2. 기술적 구현 방법
두 가지 핵심 기법:

Contextual Embeddings (맥락적 임베딩)

각 청크를 임베딩하기 전에 Claude를 사용해 50-100 토큰의 맥락 설명을 자동 생성
예시 변환:
원본: "회사의 매출이 전 분기 대비 3% 성장했다."
↓
맥락 추가: "이 내용은 ACME 회사의 2023년 2분기 SEC 신고서에서 발췌한 것으로, 
전 분기 매출은 3억 1400만 달러였다. 회사의 매출이 전 분기 대비 3% 성장했다."



Contextual BM25

BM25(키워드 매칭 알고리즘)를 사용할 때도 맥락이 추가된 청크를 인덱싱
정확한 용어 매칭과 의미적 유사성을 모두 활용



## 3. 구현 프로세스
전처리 단계:

문서를 작은 청크로 분할 (보통 수백 토큰)
Claude 3 Haiku를 사용해 각 청크에 대한 맥락 생성
맥락이 추가된 청크로 임베딩 생성 및 BM25 인덱스 구축

런타임 검색:

사용자 쿼리 입력
BM25로 정확한 매칭 검색
임베딩으로 의미적 유사성 검색
두 결과를 랭크 퓨전으로 결합
상위 K개 청크를 프롬프트에 추가

## 4. 성능 개선 결과
정량적 개선:

Contextual Embeddings만 사용: 35% 실패율 감소 (5.7% → 3.7%)
Contextual Embeddings + BM25: 49% 실패율 감소 (5.7% → 2.9%)
리랭킹 추가 시: 67% 실패율 감소 (5.7% → 1.9%)

테스트 환경:

다양한 도메인: 코드베이스, 소설, ArXiv 논문, 과학 논문
여러 임베딩 모델 테스트 (Gemini Text 004가 최고 성능)
상위 20개 청크 검색 기준

## 5. 비용 최적화: Prompt Caching 활용
Prompt Caching의 장점:

문서를 한 번만 캐시에 로드하고 재사용
레이턴시 2배 이상 감소
비용 최대 90% 절감
100만 토큰당 약 $1.02의 낮은 처리 비용

## 6. 리랭킹(Reranking) 기법
추가 성능 향상:

초기 검색으로 상위 150개 청크 획득
리랭킹 모델(Cohere reranker 사용)로 관련성 점수 부여
최종 상위 20개 청크 선택
모델에 전달하여 응답 생성

## 7. 실무 적용 고려사항
구현 시 주의점:

청크 경계 설정: 청크 크기, 경계, 겹침 정도가 성능에 영향
임베딩 모델 선택: Gemini와 Voyage가 특히 효과적
도메인별 프롬프트 커스터마이징: 특정 분야에 맞춘 프롬프트로 더 나은 결과 가능
청크 개수 최적화: 20개가 가장 효과적이었으나 사용 사례별 실험 필요

## 8. 적용 범위와 제한사항
적합한 경우:

20만 토큰 이상의 대규모 지식 베이스
정확한 정보 검색이 중요한 기술 문서, 법률 문서, 고객 지원 등

간단한 대안:

20만 토큰 미만의 작은 지식 베이스는 전체를 프롬프트에 포함하는 것이 더 효율적

## 결론
Contextual Retrieval은 RAG 시스템의 근본적인 한계인 맥락 손실 문제를 우아하게 해결한 혁신적인 방법론입니다. 특히 Claude의 Prompt Caching 기능과 결합하여 비용 효율적으로 구현할 수 있다는 점이 실무적으로 매우 의미가 있습니다.
이 기술은 기업의 지식 관리 시스템, 고객 지원 챗봇, 법률/의료 분야의 전문 AI 시스템 등에서 즉시 활용 가능하며, 검색 정확도를 획기적으로 개선할 수 있는 실용적인 솔루션입니다.